{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 11.4MB 4.1MB/s eta 0:00:01\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-1.9.6\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gamestate import GameState, start_ai_only_game\n",
    "from models.RandomModel import RandomModel\n",
    "from models.HeuristicModel import HeuristicModel\n",
    "from models.DeepRLModel import DeepRLModel, DQN, STATE_ENCODING_LENGTH\n",
    "from trainutil import train_models, test_models, get_epsilon_decay_factor\n",
    "import fsutils as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay_window calc: 194\n",
      "Learning starts at episode: 6\n",
      "Decay rate:  0.9747946518906935\n"
     ]
    }
   ],
   "source": [
    "episodes = 200\n",
    "steps = 1200 \n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "buffer_capacity = 15000\n",
    "buffer_learn_thresh = 0.5\n",
    "batch_size = 128\n",
    "\n",
    "target_update = 10\n",
    "\n",
    "start_epsilon = 0.99\n",
    "min_epsilon = 0.01\n",
    "decay_window = 180\n",
    "decay = get_epsilon_decay_factor(start_epsilon, min_epsilon, decay_window)\n",
    "\n",
    "\n",
    "calc_decay_window = episodes - int(buffer_learn_thresh * buffer_capacity / steps)\n",
    "print(\"decay_window calc:\", calc_decay_window)\n",
    "print(\"Learning starts at episode:\", int(buffer_learn_thresh * buffer_capacity / steps))\n",
    "print(\"Decay rate: \", decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=9, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc4): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQN(STATE_ENCODING_LENGTH, 8)\n",
    "fs.load_net_from_device(net, \"drl_fc32\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc1): Linear(in_features=9, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = GameState()\n",
    "deep_rl_model = DeepRLModel(\n",
    "                    epsilon = start_epsilon,\n",
    "                    min_epsilon = min_epsilon,\n",
    "                    epsilon_decay = decay,\n",
    "                    gamma = gamma,\n",
    "                    buffer_capacity = buffer_capacity,\n",
    "                    replay_buffer_learn_thresh = buffer_learn_thresh,\n",
    "                    batch_size = batch_size,\n",
    "                    lr = 1e-4,\n",
    "                    model = None)\n",
    "\n",
    "# rand_model_1 = RandomModel(min_steps=5, max_steps=10)\n",
    "# rand_model_2 = RandomModel(min_steps=5, max_steps=10)\n",
    "models = [deep_rl_model]\n",
    "\n",
    "print(deep_rl_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN MODE\n",
      "=== Starting Episode 0 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -3.464872595550389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Episode Loss: nan | Episode Reward: -5.6429 | Mean Reward: -5.6429\n",
      "=== Starting Episode 1 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -9.345746285944614\n",
      "Mean Episode Loss: nan | Episode Reward: -11.8808 | Mean Reward: -8.7618\n",
      "=== Starting Episode 2 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -4.301523534323579\n",
      "Mean Episode Loss: nan | Episode Reward: -6.6601 | Mean Reward: -8.0613\n",
      "=== Starting Episode 3 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 1.7827531000930321\n",
      "Mean Episode Loss: nan | Episode Reward: 2.3681 | Mean Reward: -5.4539\n",
      "=== Starting Episode 4 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 97.5122970706758\n",
      "Mean Episode Loss: nan | Episode Reward: 100.6416 | Mean Reward: 15.7652\n",
      "=== Starting Episode 5 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -0.41759995898217994\n",
      "Mean Episode Loss: nan | Episode Reward: -1.4267 | Mean Reward: 12.8999\n",
      "=== Starting Episode 6 ===\n",
      "----LEARNING BEGINS----\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -2.2656027622261377\n",
      "Mean Episode Loss: 1.5589 | Episode Reward: -3.8287 | Mean Reward: 10.5101\n",
      "=== Starting Episode 7 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -7.2997467961289715\n",
      "Mean Episode Loss: 0.9865 | Episode Reward: -9.6189 | Mean Reward: 7.9940\n",
      "=== Starting Episode 8 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 2.344027599051927\n",
      "Mean Episode Loss: 0.9848 | Episode Reward: 1.0762 | Mean Reward: 7.2253\n",
      "=== Starting Episode 9 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -8.462039271001203\n",
      "Mean Episode Loss: 0.6610 | Episode Reward: -11.0145 | Mean Reward: 5.4013\n",
      "=== Starting Episode 10 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -2.780129668006751\n",
      "Mean Episode Loss: 0.7888 | Episode Reward: -2.0701 | Mean Reward: 5.7586\n",
      "=== Starting Episode 11 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -12.31890837738365\n",
      "Mean Episode Loss: 0.7277 | Episode Reward: -14.0395 | Mean Reward: 5.5427\n",
      "=== Starting Episode 12 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 0.8060930582455939\n",
      "Mean Episode Loss: 0.4670 | Episode Reward: 4.1566 | Mean Reward: 6.6244\n",
      "=== Starting Episode 13 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 3.0365031640120748\n",
      "Mean Episode Loss: 0.4024 | Episode Reward: 7.4240 | Mean Reward: 7.1300\n",
      "=== Starting Episode 14 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 6.626946405727125\n",
      "Mean Episode Loss: 0.6614 | Episode Reward: 7.9396 | Mean Reward: -2.1402\n",
      "=== Starting Episode 15 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 10.520194700266039\n",
      "Mean Episode Loss: 0.4665 | Episode Reward: 9.1306 | Mean Reward: -1.0845\n",
      "=== Starting Episode 16 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -1.5649009557093194\n",
      "Mean Episode Loss: 0.1437 | Episode Reward: -3.0119 | Mean Reward: -1.0028\n",
      "=== Starting Episode 17 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -7.273137304029305\n",
      "Mean Episode Loss: 0.2710 | Episode Reward: -9.7702 | Mean Reward: -1.0179\n",
      "=== Starting Episode 18 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 5.292589035606952\n",
      "Mean Episode Loss: 0.0104 | Episode Reward: 3.5331 | Mean Reward: -0.7722\n",
      "=== Starting Episode 19 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 4.017059319790945\n",
      "Mean Episode Loss: 0.0102 | Episode Reward: 1.8887 | Mean Reward: 0.5181\n",
      "=== Starting Episode 20 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -5.585824387212753\n",
      "Mean Episode Loss: 0.0099 | Episode Reward: -7.0127 | Mean Reward: 0.0238\n",
      "=== Starting Episode 21 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 2.1965352859113807\n",
      "Mean Episode Loss: 0.0095 | Episode Reward: 1.8694 | Mean Reward: 1.6147\n",
      "=== Starting Episode 22 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 15.93766636192494\n",
      "Mean Episode Loss: 0.0111 | Episode Reward: 15.6331 | Mean Reward: 2.7624\n",
      "=== Starting Episode 23 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 1.027546057801544\n",
      "Mean Episode Loss: 0.0117 | Episode Reward: -0.7464 | Mean Reward: 1.9453\n",
      "=== Starting Episode 24 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 0.9606101460762062\n",
      "Mean Episode Loss: 0.0111 | Episode Reward: -0.1815 | Mean Reward: 1.1332\n",
      "=== Starting Episode 25 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 12.79895369599086\n",
      "Mean Episode Loss: 0.0120 | Episode Reward: 14.8719 | Mean Reward: 1.7074\n",
      "=== Starting Episode 26 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -2.4507057049199688\n",
      "Mean Episode Loss: 0.0123 | Episode Reward: -3.9077 | Mean Reward: 1.6178\n",
      "=== Starting Episode 27 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 11.557620588594723\n",
      "Mean Episode Loss: 0.0122 | Episode Reward: 11.1905 | Mean Reward: 3.7139\n",
      "=== Starting Episode 28 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -0.2220917009263843\n",
      "Mean Episode Loss: 0.0120 | Episode Reward: -1.8138 | Mean Reward: 3.1792\n",
      "=== Starting Episode 29 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -2.7356733280957712\n",
      "Mean Episode Loss: 0.0115 | Episode Reward: -5.3418 | Mean Reward: 2.4561\n",
      "=== Starting Episode 30 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: -1.6107652168912168\n",
      "Mean Episode Loss: 0.0117 | Episode Reward: -2.6628 | Mean Reward: 2.8911\n",
      "=== Starting Episode 31 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 98.05723963170782\n",
      "Mean Episode Loss: 0.5984 | Episode Reward: 113.1303 | Mean Reward: 14.0172\n",
      "=== Starting Episode 32 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 9.35641714480981\n",
      "Mean Episode Loss: 0.6013 | Episode Reward: 18.3322 | Mean Reward: 14.2871\n",
      "=== Starting Episode 33 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 9.964882060812641\n",
      "Mean Episode Loss: 0.7321 | Episode Reward: 115.4955 | Mean Reward: 25.9113\n",
      "=== Starting Episode 34 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 14.86001015026278\n",
      "Mean Episode Loss: 1.3177 | Episode Reward: 14.8982 | Mean Reward: 27.4193\n",
      "=== Starting Episode 35 ===\n",
      "----STEP 1000 rewards----\n",
      "Model 0: 15.973600621174555\n"
     ]
    }
   ],
   "source": [
    "model_name = \"drl_fc32_6\"\n",
    "train_models(env, models, episodes=episodes, steps=steps, target_update=target_update, print_every=1000, model_name=model_name)\n",
    "# test_models(env, models, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.save_net_to_disk(deep_rl_model.model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
