{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 11.4MB 4.1MB/s eta 0:00:01\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-1.9.6\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gamestate import GameState, start_ai_only_game\n",
    "from models.RandomModel import RandomModel\n",
    "from models.HeuristicModel import HeuristicModel\n",
    "from models.DeepRLModel import DeepRLModel, DQN, STATE_ENCODING_LENGTH\n",
    "from trainutil import train_models, test_models, get_epsilon_decay_factor\n",
    "import fsutils as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay_window calc: 143\n",
      "Learning starts at episode: 7\n",
      "Decay rate:  0.9705844924924611\n"
     ]
    }
   ],
   "source": [
    "episodes = 150\n",
    "steps = 1000 \n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "buffer_capacity = 15000\n",
    "buffer_learn_thresh = 0.5\n",
    "batch_size = 512\n",
    "\n",
    "target_update = 10\n",
    "\n",
    "start_epsilon = 0.99\n",
    "min_epsilon = 0.05\n",
    "decay_window = 100\n",
    "decay = get_epsilon_decay_factor(start_epsilon, min_epsilon, decay_window)\n",
    "\n",
    "\n",
    "calc_decay_window = episodes - int(buffer_learn_thresh * buffer_capacity / steps)\n",
    "print(\"decay_window calc:\", calc_decay_window)\n",
    "print(\"Learning starts at episode:\", int(buffer_learn_thresh * buffer_capacity / steps))\n",
    "print(\"Decay rate: \", decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=9, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc4): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQN(STATE_ENCODING_LENGTH, 8)\n",
    "fs.load_net_from_device(net, \"drl_fc32\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc1): Linear(in_features=9, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = GameState()\n",
    "deep_rl_model = DeepRLModel(\n",
    "                    epsilon = start_epsilon,\n",
    "                    min_epsilon = min_epsilon,\n",
    "                    epsilon_decay = decay,\n",
    "                    gamma = gamma,\n",
    "                    buffer_capacity = buffer_capacity,\n",
    "                    replay_buffer_learn_thresh = buffer_learn_thresh,\n",
    "                    batch_size = batch_size,\n",
    "                    lr = 1e-3,\n",
    "                    model = None)\n",
    "\n",
    "# rand_model_1 = RandomModel(min_steps=5, max_steps=10)\n",
    "# rand_model_2 = RandomModel(min_steps=5, max_steps=10)\n",
    "models = [deep_rl_model]\n",
    "\n",
    "print(deep_rl_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN MODE\n",
      "=== Starting Episode 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Episode Loss: nan | Episode Reward: 1.6683 | Mean Reward: 1.6683\n",
      "=== Starting Episode 1 ===\n",
      "Mean Episode Loss: nan | Episode Reward: -5.9162 | Mean Reward: -2.1240\n",
      "=== Starting Episode 2 ===\n",
      "Mean Episode Loss: nan | Episode Reward: -5.8944 | Mean Reward: -3.3808\n",
      "=== Starting Episode 3 ===\n",
      "Mean Episode Loss: nan | Episode Reward: -2.7780 | Mean Reward: -3.2301\n",
      "=== Starting Episode 4 ===\n",
      "Mean Episode Loss: nan | Episode Reward: -2.7493 | Mean Reward: -3.1339\n",
      "=== Starting Episode 5 ===\n",
      "Mean Episode Loss: nan | Episode Reward: -5.6270 | Mean Reward: -3.5494\n",
      "=== Starting Episode 6 ===\n",
      "Mean Episode Loss: nan | Episode Reward: 1.9080 | Mean Reward: -2.7698\n",
      "=== Starting Episode 7 ===\n",
      "----LEARNING BEGINS----\n",
      "Mean Episode Loss: 0.0181 | Episode Reward: 2.4975 | Mean Reward: -2.1114\n",
      "=== Starting Episode 8 ===\n",
      "Mean Episode Loss: 0.0080 | Episode Reward: -2.0458 | Mean Reward: -2.1041\n",
      "=== Starting Episode 9 ===\n",
      "Mean Episode Loss: 0.0089 | Episode Reward: -1.4666 | Mean Reward: -2.0403\n",
      "=== Starting Episode 10 ===\n",
      "Mean Episode Loss: 0.0090 | Episode Reward: 0.1291 | Mean Reward: -2.1943\n",
      "=== Starting Episode 11 ===\n",
      "Mean Episode Loss: 0.0086 | Episode Reward: 2.2403 | Mean Reward: -1.3786\n",
      "=== Starting Episode 12 ===\n",
      "Mean Episode Loss: 0.0087 | Episode Reward: -5.0727 | Mean Reward: -1.2964\n",
      "=== Starting Episode 13 ===\n",
      "Mean Episode Loss: 0.0092 | Episode Reward: 7.0972 | Mean Reward: -0.3089\n",
      "=== Starting Episode 14 ===\n",
      "Mean Episode Loss: 0.0093 | Episode Reward: -2.2052 | Mean Reward: -0.2545\n",
      "=== Starting Episode 15 ===\n",
      "Mean Episode Loss: 0.0090 | Episode Reward: -2.1798 | Mean Reward: 0.0902\n",
      "=== Starting Episode 16 ===\n",
      "Mean Episode Loss: 0.0088 | Episode Reward: -4.5168 | Mean Reward: -0.5523\n",
      "=== Starting Episode 17 ===\n",
      "Mean Episode Loss: 0.0088 | Episode Reward: -5.3546 | Mean Reward: -1.3375\n",
      "=== Starting Episode 18 ===\n",
      "Mean Episode Loss: 0.0088 | Episode Reward: -6.1425 | Mean Reward: -1.7472\n",
      "=== Starting Episode 19 ===\n",
      "Mean Episode Loss: 0.0087 | Episode Reward: -1.3794 | Mean Reward: -1.7384\n",
      "=== Starting Episode 20 ===\n",
      "Mean Episode Loss: 0.0083 | Episode Reward: -5.6290 | Mean Reward: -2.3142\n",
      "=== Starting Episode 21 ===\n",
      "Mean Episode Loss: 0.0086 | Episode Reward: 3.7218 | Mean Reward: -2.1661\n",
      "=== Starting Episode 22 ===\n",
      "Mean Episode Loss: 0.0091 | Episode Reward: -4.8926 | Mean Reward: -2.1481\n",
      "=== Starting Episode 23 ===\n",
      "Mean Episode Loss: 0.0092 | Episode Reward: -2.1853 | Mean Reward: -3.0763\n",
      "=== Starting Episode 24 ===\n",
      "Mean Episode Loss: 0.0096 | Episode Reward: 12.4423 | Mean Reward: -1.6116\n",
      "=== Starting Episode 25 ===\n",
      "Mean Episode Loss: 0.0108 | Episode Reward: 20.2804 | Mean Reward: 0.6344\n",
      "=== Starting Episode 26 ===\n",
      "Mean Episode Loss: 0.0104 | Episode Reward: -8.2454 | Mean Reward: 0.2616\n",
      "=== Starting Episode 27 ===\n",
      "Mean Episode Loss: 0.0110 | Episode Reward: 13.3419 | Mean Reward: 2.1312\n",
      "=== Starting Episode 28 ===\n",
      "Mean Episode Loss: 0.0122 | Episode Reward: 13.6664 | Mean Reward: 4.1121\n",
      "=== Starting Episode 29 ===\n",
      "Mean Episode Loss: 0.0129 | Episode Reward: 0.8321 | Mean Reward: 4.3333\n",
      "=== Starting Episode 30 ===\n",
      "Mean Episode Loss: 0.0145 | Episode Reward: 26.2355 | Mean Reward: 7.5197\n",
      "=== Starting Episode 31 ===\n",
      "Mean Episode Loss: 0.0163 | Episode Reward: -1.6138 | Mean Reward: 6.9861\n",
      "=== Starting Episode 32 ===\n",
      "Mean Episode Loss: 0.0160 | Episode Reward: 8.0324 | Mean Reward: 8.2786\n",
      "=== Starting Episode 33 ===\n",
      "Mean Episode Loss: 0.0181 | Episode Reward: 23.1751 | Mean Reward: 10.8147\n",
      "=== Starting Episode 34 ===\n",
      "Mean Episode Loss: 0.0193 | Episode Reward: 5.1496 | Mean Reward: 10.0854\n",
      "=== Starting Episode 35 ===\n",
      "Mean Episode Loss: 0.4707 | Episode Reward: 129.4657 | Mean Reward: 21.0039\n",
      "=== Starting Episode 36 ===\n",
      "Mean Episode Loss: 0.5687 | Episode Reward: 17.7512 | Mean Reward: 23.6036\n",
      "=== Starting Episode 37 ===\n",
      "Mean Episode Loss: 1.4645 | Episode Reward: 117.3792 | Mean Reward: 34.0073\n",
      "=== Starting Episode 38 ===\n",
      "Mean Episode Loss: 0.9223 | Episode Reward: 14.2380 | Mean Reward: 34.0645\n",
      "=== Starting Episode 39 ===\n",
      "Mean Episode Loss: 1.4837 | Episode Reward: 11.9075 | Mean Reward: 35.1720\n",
      "=== Starting Episode 40 ===\n",
      "Mean Episode Loss: 1.4420 | Episode Reward: 26.2314 | Mean Reward: 35.1716\n",
      "=== Starting Episode 41 ===\n",
      "Mean Episode Loss: 1.3497 | Episode Reward: 20.3523 | Mean Reward: 37.3682\n",
      "=== Starting Episode 42 ===\n",
      "Mean Episode Loss: 1.3247 | Episode Reward: 28.8677 | Mean Reward: 39.4518\n",
      "=== Starting Episode 43 ===\n",
      "Mean Episode Loss: 1.3199 | Episode Reward: 10.4536 | Mean Reward: 38.1796\n",
      "=== Starting Episode 44 ===\n",
      "Mean Episode Loss: 1.2535 | Episode Reward: 8.9963 | Mean Reward: 38.5643\n",
      "=== Starting Episode 45 ===\n",
      "Mean Episode Loss: 0.9882 | Episode Reward: 27.4054 | Mean Reward: 28.3583\n",
      "=== Starting Episode 46 ===\n",
      "Mean Episode Loss: 1.1939 | Episode Reward: 111.3694 | Mean Reward: 37.7201\n",
      "=== Starting Episode 47 ===\n",
      "Mean Episode Loss: 1.8020 | Episode Reward: 11.0866 | Mean Reward: 27.0908\n",
      "=== Starting Episode 48 ===\n",
      "Mean Episode Loss: 2.3358 | Episode Reward: 2.7509 | Mean Reward: 25.9421\n",
      "=== Starting Episode 49 ===\n",
      "Mean Episode Loss: 2.0009 | Episode Reward: 16.8510 | Mean Reward: 26.4365\n",
      "=== Starting Episode 50 ===\n",
      "Mean Episode Loss: 1.6516 | Episode Reward: -1.0512 | Mean Reward: 23.7082\n",
      "=== Starting Episode 51 ===\n",
      "Mean Episode Loss: 1.1787 | Episode Reward: 17.6553 | Mean Reward: 23.4385\n",
      "=== Starting Episode 52 ===\n",
      "Mean Episode Loss: 0.6863 | Episode Reward: 8.1956 | Mean Reward: 21.3713\n",
      "=== Starting Episode 53 ===\n",
      "Mean Episode Loss: 0.3984 | Episode Reward: -1.4099 | Mean Reward: 20.1849\n",
      "=== Starting Episode 54 ===\n",
      "Mean Episode Loss: 0.4907 | Episode Reward: 0.2177 | Mean Reward: 19.3071\n",
      "=== Starting Episode 55 ===\n",
      "Mean Episode Loss: 0.4291 | Episode Reward: 19.0621 | Mean Reward: 18.4728\n",
      "=== Starting Episode 56 ===\n",
      "Mean Episode Loss: 0.4036 | Episode Reward: 22.0730 | Mean Reward: 9.5431\n",
      "=== Starting Episode 57 ===\n",
      "Mean Episode Loss: 0.6597 | Episode Reward: 156.4903 | Mean Reward: 24.0835\n",
      "=== Starting Episode 58 ===\n",
      "Mean Episode Loss: 1.3599 | Episode Reward: 10.5822 | Mean Reward: 24.8666\n",
      "=== Starting Episode 59 ===\n",
      "Mean Episode Loss: 1.7890 | Episode Reward: 86.1973 | Mean Reward: 31.8012\n",
      "=== Starting Episode 60 ===\n",
      "Mean Episode Loss: 1.8575 | Episode Reward: 6.3089 | Mean Reward: 32.5373\n",
      "=== Starting Episode 61 ===\n",
      "Mean Episode Loss: 1.7634 | Episode Reward: 15.7765 | Mean Reward: 32.3494\n",
      "=== Starting Episode 62 ===\n",
      "Mean Episode Loss: 1.3831 | Episode Reward: 7.7102 | Mean Reward: 32.3008\n",
      "=== Starting Episode 63 ===\n",
      "Mean Episode Loss: 1.1272 | Episode Reward: 23.4495 | Mean Reward: 34.7868\n",
      "=== Starting Episode 64 ===\n",
      "Mean Episode Loss: 1.0883 | Episode Reward: -1.3042 | Mean Reward: 34.6346\n",
      "=== Starting Episode 65 ===\n",
      "Mean Episode Loss: 1.8257 | Episode Reward: 144.1645 | Mean Reward: 47.1448\n",
      "=== Starting Episode 66 ===\n",
      "Mean Episode Loss: 1.8339 | Episode Reward: -8.4297 | Mean Reward: 44.0946\n",
      "=== Starting Episode 67 ===\n",
      "Mean Episode Loss: 1.7404 | Episode Reward: 12.7644 | Mean Reward: 29.7220\n",
      "=== Starting Episode 68 ===\n",
      "Mean Episode Loss: 1.5260 | Episode Reward: -1.9758 | Mean Reward: 28.4662\n",
      "=== Starting Episode 69 ===\n",
      "Mean Episode Loss: 1.5737 | Episode Reward: 2.5635 | Mean Reward: 20.1028\n",
      "=== Starting Episode 70 ===\n",
      "Mean Episode Loss: 1.5037 | Episode Reward: 4.2155 | Mean Reward: 19.8934\n",
      "=== Starting Episode 71 ===\n",
      "Mean Episode Loss: 2.2154 | Episode Reward: 9.6105 | Mean Reward: 19.2768\n",
      "=== Starting Episode 72 ===\n",
      "Mean Episode Loss: 1.6992 | Episode Reward: 0.1171 | Mean Reward: 18.5175\n",
      "=== Starting Episode 73 ===\n",
      "Mean Episode Loss: 0.9158 | Episode Reward: 12.8529 | Mean Reward: 17.4579\n",
      "=== Starting Episode 74 ===\n",
      "Mean Episode Loss: 0.1985 | Episode Reward: 6.3018 | Mean Reward: 18.2185\n",
      "=== Starting Episode 75 ===\n",
      "Mean Episode Loss: 1.0297 | Episode Reward: 126.9938 | Mean Reward: 16.5014\n",
      "=== Starting Episode 76 ===\n",
      "Mean Episode Loss: 2.4137 | Episode Reward: 158.2854 | Mean Reward: 33.1729\n",
      "=== Starting Episode 77 ===\n",
      "Mean Episode Loss: 2.3121 | Episode Reward: 25.6619 | Mean Reward: 34.4627\n",
      "=== Starting Episode 78 ===\n",
      "Mean Episode Loss: 2.4429 | Episode Reward: -2.6177 | Mean Reward: 34.3985\n",
      "=== Starting Episode 79 ===\n",
      "Mean Episode Loss: 2.3381 | Episode Reward: 11.9067 | Mean Reward: 35.3328\n",
      "=== Starting Episode 80 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Episode Loss: 2.2489 | Episode Reward: 3.2049 | Mean Reward: 35.2317\n",
      "=== Starting Episode 81 ===\n",
      "Mean Episode Loss: 1.8959 | Episode Reward: 100.1832 | Mean Reward: 44.2890\n",
      "=== Starting Episode 82 ===\n",
      "Mean Episode Loss: 2.1134 | Episode Reward: 11.0122 | Mean Reward: 45.3785\n",
      "=== Starting Episode 83 ===\n",
      "Mean Episode Loss: 1.8964 | Episode Reward: 14.9116 | Mean Reward: 45.5844\n",
      "=== Starting Episode 84 ===\n",
      "Mean Episode Loss: 2.2237 | Episode Reward: -0.5257 | Mean Reward: 44.9016\n",
      "=== Starting Episode 85 ===\n",
      "Mean Episode Loss: 1.9321 | Episode Reward: 21.7082 | Mean Reward: 34.3731\n",
      "=== Starting Episode 86 ===\n",
      "Mean Episode Loss: 2.0551 | Episode Reward: 18.3965 | Mean Reward: 20.3842\n",
      "=== Starting Episode 87 ===\n",
      "Mean Episode Loss: 1.8820 | Episode Reward: 10.8707 | Mean Reward: 18.9050\n",
      "=== Starting Episode 88 ===\n",
      "Mean Episode Loss: 2.1314 | Episode Reward: 16.3464 | Mean Reward: 20.8015\n",
      "=== Starting Episode 89 ===\n",
      "Mean Episode Loss: 3.1067 | Episode Reward: 112.9585 | Mean Reward: 30.9066\n",
      "=== Starting Episode 90 ===\n",
      "Mean Episode Loss: 2.5491 | Episode Reward: 107.3784 | Mean Reward: 41.3240\n",
      "=== Starting Episode 91 ===\n",
      "Mean Episode Loss: 2.2362 | Episode Reward: -8.7773 | Mean Reward: 30.4279\n",
      "=== Starting Episode 92 ===\n",
      "Mean Episode Loss: 1.6695 | Episode Reward: 16.0613 | Mean Reward: 30.9329\n",
      "=== Starting Episode 93 ===\n",
      "Mean Episode Loss: 1.7053 | Episode Reward: 122.2061 | Mean Reward: 41.6623\n",
      "=== Starting Episode 94 ===\n",
      "Mean Episode Loss: 2.5710 | Episode Reward: 1.6881 | Mean Reward: 41.8837\n",
      "=== Starting Episode 95 ===\n",
      "Mean Episode Loss: 2.3171 | Episode Reward: 135.0446 | Mean Reward: 53.2173\n",
      "=== Starting Episode 96 ===\n",
      "Mean Episode Loss: 2.1675 | Episode Reward: 0.1445 | Mean Reward: 51.3921\n",
      "=== Starting Episode 97 ===\n",
      "Mean Episode Loss: 2.0393 | Episode Reward: 15.1003 | Mean Reward: 51.8151\n",
      "=== Starting Episode 98 ===\n",
      "Mean Episode Loss: 1.8336 | Episode Reward: 30.3512 | Mean Reward: 53.2156\n",
      "=== Starting Episode 99 ===\n",
      "Mean Episode Loss: 1.8605 | Episode Reward: 5.0711 | Mean Reward: 42.4268\n",
      "=== Starting Episode 100 ===\n",
      "Mean Episode Loss: 1.7202 | Episode Reward: 16.4678 | Mean Reward: 33.3358\n",
      "=== Starting Episode 101 ===\n",
      "Mean Episode Loss: 3.9852 | Episode Reward: 26.9477 | Mean Reward: 36.9083\n",
      "=== Starting Episode 102 ===\n",
      "Mean Episode Loss: 2.6077 | Episode Reward: 33.3455 | Mean Reward: 38.6367\n",
      "=== Starting Episode 103 ===\n",
      "Mean Episode Loss: 2.5971 | Episode Reward: 11.4103 | Mean Reward: 27.5571\n",
      "=== Starting Episode 104 ===\n",
      "Mean Episode Loss: 2.0271 | Episode Reward: 25.4092 | Mean Reward: 29.9292\n",
      "=== Starting Episode 105 ===\n",
      "Mean Episode Loss: 1.1618 | Episode Reward: 24.4599 | Mean Reward: 18.8708\n",
      "=== Starting Episode 106 ===\n",
      "Mean Episode Loss: 0.7917 | Episode Reward: 21.9568 | Mean Reward: 21.0520\n",
      "=== Starting Episode 107 ===\n",
      "Mean Episode Loss: 0.8446 | Episode Reward: 0.3801 | Mean Reward: 19.5800\n",
      "=== Starting Episode 108 ===\n",
      "Mean Episode Loss: 0.9374 | Episode Reward: 22.5857 | Mean Reward: 18.8034\n",
      "=== Starting Episode 109 ===\n",
      "Mean Episode Loss: 0.5538 | Episode Reward: 16.5744 | Mean Reward: 19.9537\n",
      "=== Starting Episode 110 ===\n",
      "Mean Episode Loss: 0.2899 | Episode Reward: 17.2051 | Mean Reward: 20.0275\n",
      "=== Starting Episode 111 ===\n",
      "Mean Episode Loss: 0.0346 | Episode Reward: 34.1978 | Mean Reward: 20.7525\n",
      "=== Starting Episode 112 ===\n",
      "Mean Episode Loss: 0.0339 | Episode Reward: 19.3069 | Mean Reward: 19.3486\n",
      "=== Starting Episode 113 ===\n",
      "Mean Episode Loss: 0.0343 | Episode Reward: 19.9052 | Mean Reward: 20.1981\n",
      "=== Starting Episode 114 ===\n",
      "Mean Episode Loss: 0.0328 | Episode Reward: 8.7813 | Mean Reward: 18.5353\n",
      "=== Starting Episode 115 ===\n"
     ]
    }
   ],
   "source": [
    "model_name = \"drl_fc32_8\"\n",
    "train_models(env, models, episodes=episodes, steps=steps, target_update=target_update, print_every=1000, model_name=model_name)\n",
    "# test_models(env, models, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.save_net_to_disk(deep_rl_model.model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
